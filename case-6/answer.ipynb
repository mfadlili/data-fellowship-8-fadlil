{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF8 Case 6 Solutions\n",
    "\n",
    "Muhammad Fadlil Ismail\n",
    "\n",
    "## Problems\n",
    "\n",
    "Download the February 2021 data from TLC Trip Record website\n",
    "\n",
    "(https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and use\n",
    "\n",
    "Pyspark to analyze and answer these questions below. Upload your script\n",
    "\n",
    "into Github or Gdrive.\n",
    "\n",
    "## Import Library and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/48/cc321e742a93320c681b3c7a9fd405d518c6326c89da3d67e35b9868e941/pyspark-3.3.1.tar.gz (281.4MB)\n",
      "Collecting py4j==0.10.9.5 (from pyspark)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/ec/60880978512d5569ca4bf32b3b4d7776a528ecf4bca4523936c98c92a3c8/py4j-0.10.9.5-py2.py3-none-any.whl (199kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Muhammad Fadlil\\AppData\\Local\\pip\\Cache\\wheels\\e5\\cc\\9a\\0c20ee0940a9e80053edfe2270daee438f36037e1ff041c0ec\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"fhv_tripdata_2021-02.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create view from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"ParquetTable\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00013|2021-02-01 07:01:00|2021-02-01 08:33:00|        null|        null|   null|                B00014|\n",
      "|     B00021         |2021-02-01 07:55:40|2021-02-01 08:06:20|       173.0|        82.0|   null|       B00021         |\n",
      "|     B00021         |2021-02-01 07:14:03|2021-02-01 07:28:37|       173.0|        56.0|   null|       B00021         |\n",
      "|     B00021         |2021-02-01 07:27:48|2021-02-01 07:35:45|        82.0|       129.0|   null|       B00021         |\n",
      "|              B00037|2021-02-01 07:12:50|2021-02-01 07:26:38|        null|       225.0|   null|                B00037|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from ParquetTable limit 5\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 : How many taxi trips were there on February 15?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|pickup_datetime|count(1)|\n",
      "+---------------+--------+\n",
      "|     2021-02-15|   35709|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select cast(pickup_datetime as date), COUNT(*) from ParquetTable where extract(day from pickup_datetime) = 15 group by 1\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 : Find the longest trip for each day ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|pickup_datetime|time_delta_minutes|\n",
      "+---------------+------------------+\n",
      "|     2021-02-01|           46290.0|\n",
      "|     2021-02-02|1390.7833333333333|\n",
      "|     2021-02-03|1246.1666666666667|\n",
      "|     2021-02-04| 40034.88333333333|\n",
      "|     2021-02-05|          110919.0|\n",
      "|     2021-02-06| 2752.633333333333|\n",
      "|     2021-02-07|1306.1166666666666|\n",
      "|     2021-02-08| 9424.916666666666|\n",
      "|     2021-02-09|1459.9833333333333|\n",
      "|     2021-02-10|1407.7166666666667|\n",
      "|     2021-02-11|3219.8166666666666|\n",
      "|     2021-02-12|            4344.0|\n",
      "|     2021-02-13| 8422.683333333332|\n",
      "|     2021-02-14|            1519.4|\n",
      "|     2021-02-15|          14670.15|\n",
      "|     2021-02-16|            4816.1|\n",
      "|     2021-02-17| 4284.783333333334|\n",
      "|     2021-02-18|2749.0333333333333|\n",
      "|     2021-02-19|           9012.15|\n",
      "|     2021-02-20|2701.4666666666667|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select cast(pickup_datetime as date), max((bigint(to_timestamp(dropOff_datetime))-bigint(to_timestamp(pickup_datetime)))/60) as time_delta_minutes from ParquetTable group by 1 order by 1\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 : Find Top 5 Most frequent `dispatching_base_num` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------+\n",
      "|dispatching_base_num|number_of_dispatching_base_num|\n",
      "+--------------------+------------------------------+\n",
      "|              B00856|                         35077|\n",
      "|              B01312|                         33089|\n",
      "|              B01145|                         31114|\n",
      "|              B02794|                         30397|\n",
      "|              B03016|                         29794|\n",
      "+--------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select dispatching_base_num, count(*) as number_of_dispatching_base_num from ParquetTable group by 1 order by 2 desc limit 5\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 : Find Top 5 Most common location pairs (PUlocationID and DOlocationID)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------+\n",
      "|PUlocationID|DOlocationID|countPUDUpairs|\n",
      "+------------+------------+--------------+\n",
      "|       206.0|       206.0|          2374|\n",
      "|       221.0|       206.0|          2112|\n",
      "|       129.0|       129.0|          1902|\n",
      "|         7.0|         7.0|          1829|\n",
      "|       179.0|       179.0|          1736|\n",
      "+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select PUlocationID, DOlocationID, count(*) as countPUDUpairs from ParquetTable where PUlocationID is not null and DOlocationID is not null group by 1,2 order by 3 desc limit 5\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
